{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nimport seaborn as sns\nimport tqdm\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nfrom scipy import spatial\nimport scipy\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T20:18:33.910242Z","iopub.execute_input":"2022-06-17T20:18:33.910640Z","iopub.status.idle":"2022-06-17T20:18:36.661117Z","shell.execute_reply.started":"2022-06-17T20:18:33.910590Z","shell.execute_reply":"2022-06-17T20:18:36.660321Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/medium-articles/medium_articles.csv\")\nprint(f\"Data has {len(data)} rows\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T20:18:38.906861Z","iopub.execute_input":"2022-06-17T20:18:38.907511Z","iopub.status.idle":"2022-06-17T20:18:52.683652Z","shell.execute_reply.started":"2022-06-17T20:18:38.907473Z","shell.execute_reply":"2022-06-17T20:18:52.682725Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Data has 192368 rows\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                              title  \\\n0               Mental Note Vol. 24   \n1         Your Brain On Coronavirus   \n2                    Mind Your Nose   \n3          The 4 Purposes of Dreams   \n4  Surviving a Rod Through the Head   \n\n                                                text  \\\n0  Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...   \n1  Your Brain On Coronavirus\\n\\nA guide to the cu...   \n2  Mind Your Nose\\n\\nHow smell training can chang...   \n3  Passionate about the synergy between science a...   \n4  Youâ€™ve heard of him, havenâ€™t you? Phineas Gage...   \n\n                                                 url                 authors  \\\n0  https://medium.com/invisible-illness/mental-no...            ['Ryan Fan']   \n1  https://medium.com/age-of-awareness/how-the-pa...       ['Simon Spichak']   \n2  https://medium.com/neodotlife/mind-your-nose-f...                      []   \n3  https://medium.com/science-for-real/the-4-purp...  ['Eshan Samaranayake']   \n4  https://medium.com/live-your-life-on-purpose/s...        ['Rishav Sinha']   \n\n                          timestamp  \\\n0  2020-12-26 03:38:10.479000+00:00   \n1  2020-09-23 22:10:17.126000+00:00   \n2  2020-10-10 20:17:37.132000+00:00   \n3  2020-12-21 16:05:19.524000+00:00   \n4  2020-02-26 00:01:01.576000+00:00   \n\n                                                tags  \n0  ['Mental Health', 'Health', 'Psychology', 'Sci...  \n1  ['Mental Health', 'Coronavirus', 'Science', 'P...  \n2  ['Biotechnology', 'Neuroscience', 'Brain', 'We...  \n3  ['Health', 'Neuroscience', 'Mental Health', 'P...  \n4  ['Brain', 'Health', 'Development', 'Psychology...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n      <th>url</th>\n      <th>authors</th>\n      <th>timestamp</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mental Note Vol. 24</td>\n      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n      <td>https://medium.com/invisible-illness/mental-no...</td>\n      <td>['Ryan Fan']</td>\n      <td>2020-12-26 03:38:10.479000+00:00</td>\n      <td>['Mental Health', 'Health', 'Psychology', 'Sci...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Your Brain On Coronavirus</td>\n      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n      <td>https://medium.com/age-of-awareness/how-the-pa...</td>\n      <td>['Simon Spichak']</td>\n      <td>2020-09-23 22:10:17.126000+00:00</td>\n      <td>['Mental Health', 'Coronavirus', 'Science', 'P...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Mind Your Nose</td>\n      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n      <td>https://medium.com/neodotlife/mind-your-nose-f...</td>\n      <td>[]</td>\n      <td>2020-10-10 20:17:37.132000+00:00</td>\n      <td>['Biotechnology', 'Neuroscience', 'Brain', 'We...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The 4 Purposes of Dreams</td>\n      <td>Passionate about the synergy between science a...</td>\n      <td>https://medium.com/science-for-real/the-4-purp...</td>\n      <td>['Eshan Samaranayake']</td>\n      <td>2020-12-21 16:05:19.524000+00:00</td>\n      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Surviving a Rod Through the Head</td>\n      <td>Youâ€™ve heard of him, havenâ€™t you? Phineas Gage...</td>\n      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n      <td>['Rishav Sinha']</td>\n      <td>2020-02-26 00:01:01.576000+00:00</td>\n      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def RemoveNewLines(text):\n    text = text.replace(\"\\n\", \" \")\n    return text\n\ndef CheckTag(tags):\n    for tag in ['Science', \"Machine Learning\", \"Artificial Intelligence\", \"Health\", \"Coronavirus\"]:\n        if tag in eval(tags):\n            return True\n    return False\n\ndata[\"contains_tag\"] = data[\"tags\"].progress_apply(CheckTag)\ndata = data[data[\"contains_tag\"] == True]\ndata[\"text\"] = data[\"text\"].progress_apply(RemoveNewLines)\ndata[\"tokens\"] = data[\"text\"].progress_apply(str.split)\ndata[\"text_len\"] = data[\"tokens\"].progress_apply(len)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T20:18:52.685482Z","iopub.execute_input":"2022-06-17T20:18:52.685859Z","iopub.status.idle":"2022-06-17T20:19:05.446544Z","shell.execute_reply.started":"2022-06-17T20:18:52.685823Z","shell.execute_reply":"2022-06-17T20:19:05.445746Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/192368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af69c6adba44ffaa8fce7c2b41b8c3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14260 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"217aefd7d20e408fa19a5e8e046234da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14260 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fd56975efc3422bb0370730bb3d0a87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14260 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b1b0a1ba40247d8a50e63a05100505d"}},"metadata":{}}]},{"cell_type":"code","source":"with open(\"corpus.txt\", \"w\") as f:\n    for text in data[\"text\"].values:\n        f.write(text+\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-17T20:19:05.447918Z","iopub.execute_input":"2022-06-17T20:19:05.448456Z","iopub.status.idle":"2022-06-17T20:19:05.733626Z","shell.execute_reply.started":"2022-06-17T20:19:05.448418Z","shell.execute_reply":"2022-06-17T20:19:05.732584Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(\"../input/huggingface-roberta-variants/distilroberta-base/distilroberta-base\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"../input/huggingface-roberta-variants/distilroberta-base/distilroberta-base\").to(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T20:19:05.737070Z","iopub.execute_input":"2022-06-17T20:19:05.737424Z","iopub.status.idle":"2022-06-17T20:19:09.187602Z","shell.execute_reply.started":"2022-06-17T20:19:05.737394Z","shell.execute_reply":"2022-06-17T20:19:09.186729Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset = LineByLineTextDataset(tokenizer=tokenizer,\n                                file_path=\"corpus.txt\",\n                                block_size=128)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T20:19:09.188785Z","iopub.execute_input":"2022-06-17T20:19:09.189249Z","iopub.status.idle":"2022-06-17T20:20:09.884237Z","shell.execute_reply.started":"2022-06-17T20:19:09.189212Z","shell.execute_reply":"2022-06-17T20:20:09.883336Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n                                                mlm=True,\n                                                mlm_probability=0.15)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T20:20:09.888020Z","iopub.execute_input":"2022-06-17T20:20:09.888555Z","iopub.status.idle":"2022-06-17T20:20:09.892621Z","shell.execute_reply.started":"2022-06-17T20:20:09.888515Z","shell.execute_reply":"2022-06-17T20:20:09.891896Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=TrainingArguments(\n        output_dir=\"./\",\n        overwrite_output_dir=True,\n        num_train_epochs=5,\n        per_device_train_batch_size=64,\n        save_steps=200,\n        save_total_limit=2,\n    ),\n    data_collator=data_collator,\n    train_dataset=dataset,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T20:20:09.893902Z","iopub.execute_input":"2022-06-17T20:20:09.894423Z","iopub.status.idle":"2022-06-17T20:32:27.348973Z","shell.execute_reply.started":"2022-06-17T20:20:09.894388Z","shell.execute_reply":"2022-06-17T20:32:27.347972Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14260\n  Num Epochs = 5\n  Instantaneous batch size per device = 64\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1115\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mryanbarretto\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.12.18 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.16"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20220617_202012-3malj3s6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/ryanbarretto/huggingface/runs/3malj3s6\" target=\"_blank\">./</a></strong> to <a href=\"https://wandb.ai/ryanbarretto/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1115' max='1115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1115/1115 12:09, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.938600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.829100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./checkpoint-200\nConfiguration saved in ./checkpoint-200/config.json\nModel weights saved in ./checkpoint-200/pytorch_model.bin\nSaving model checkpoint to ./checkpoint-400\nConfiguration saved in ./checkpoint-400/config.json\nModel weights saved in ./checkpoint-400/pytorch_model.bin\nSaving model checkpoint to ./checkpoint-600\nConfiguration saved in ./checkpoint-600/config.json\nModel weights saved in ./checkpoint-600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-200] due to args.save_total_limit\nSaving model checkpoint to ./checkpoint-800\nConfiguration saved in ./checkpoint-800/config.json\nModel weights saved in ./checkpoint-800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-400] due to args.save_total_limit\nSaving model checkpoint to ./checkpoint-1000\nConfiguration saved in ./checkpoint-1000/config.json\nModel weights saved in ./checkpoint-1000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-600] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1115, training_loss=1.8757108953501611, metrics={'train_runtime': 736.208, 'train_samples_per_second': 96.848, 'train_steps_per_second': 1.515, 'total_flos': 2363983702502400.0, 'train_loss': 1.8757108953501611, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}